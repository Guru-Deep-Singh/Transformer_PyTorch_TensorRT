{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47f67ede",
   "metadata": {},
   "source": [
    "# Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac52c563",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guru/git_reps/transformer_pytorch/myenv_jetson/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from config import get_config, get_weights_file_path\n",
    "from train import get_model, get_ds, run_validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2098d6eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Max length of source sentence: 466\n",
      "Max length of target sentence: 479\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device {device}')\n",
    "config = get_config()\n",
    "train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38bfdf99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model(config, tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "# Load the pretrained weights\n",
    "model_filename = get_weights_file_path(config, f\"10\")\n",
    "state = torch.load(model_filename)\n",
    "model.load_state_dict(state['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34b7ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5bb38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading TRT Engines...\n",
      "Engines loaded.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<run_trt_split.TRTTransformer at 0xfffeea8cdf00>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTANT: this import should NOT trigger inference because the file uses if __name__ == \"__main__\"\n",
    "from run_trt_split import TRTTransformer, greedy_decode as greedy_decode_trt\n",
    "\n",
    "# (optional) same memory fraction trick in notebook\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.set_per_process_memory_fraction(0.6)\n",
    "\n",
    "trt_model = TRTTransformer(\n",
    "    enc_path=\"tensorrt_split/tmodel_10_encoder_fp32.engine\",\n",
    "    dec_path=\"tensorrt_split/tmodel_10_decoder_fp32.engine\",\n",
    "    proj_path=\"tensorrt_split/tmodel_10_projection_fp32.engine\"\n",
    ")\n",
    "\n",
    "trt_model  # sanity: should print \"Engines loaded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f431ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import greedy_decode as greedy_decode_pt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "007a9e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_decode(\n",
    "    decode_fn,\n",
    "    model_obj,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"\",\n",
    "    print_warmup_samples=0   # <-- NEW\n",
    "):\n",
    "    times_ms = []\n",
    "\n",
    "    # ---------------- Warmup (optional prints here) ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= warmup_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        out_ids = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "        if i < print_warmup_samples:\n",
    "            out_text = tokenizer_tgt.decode(out_ids.detach().cpu().numpy())\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{label} WARMUP SAMPLE {i+1}\")\n",
    "            print(f\"SOURCE:    {batch['src_text'][0]}\")\n",
    "            print(f\"TARGET:    {batch['tgt_text'][0]}\")\n",
    "            print(f\"PRED:      {out_text}\")\n",
    "\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "    # ---------------- Timed runs (no printing) ----------------\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        if i >= n_batches:\n",
    "            break\n",
    "\n",
    "        src = batch[\"encoder_input\"].to(device)\n",
    "        src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        _ = decode_fn(model_obj, src, src_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        times_ms.append((t1 - t0) * 1000)\n",
    "\n",
    "    arr = np.array(times_ms, dtype=np.float32)\n",
    "    print(f\"\\n{label} latency over {len(arr)} batches:\")\n",
    "    print(f\"  mean: {arr.mean():.2f} ms\")\n",
    "    print(f\"  p50 : {np.percentile(arr, 50):.2f} ms\")\n",
    "    print(f\"  p90 : {np.percentile(arr, 90):.2f} ms\")\n",
    "    print(f\"  p99 : {np.percentile(arr, 99):.2f} ms\")\n",
    "\n",
    "    return times_ms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "574d67ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n",
      "NvMapMemAllocInternalTagged: 1075072515 error 12\n",
      "NvMapMemHandleAlloc: error 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 1\n",
      "SOURCE:    \"Is there then no possibility of sparing these two their beating?\" he asked him.\n",
      "TARGET:    »Gibt es keine Möglichkeit, den beiden die Prügel zu ersparen?« fragte er ihn.\n",
      "PRED:      » Ist denn denn denn denn nicht die Möglichkeit , die er hier verlassen ?« fragte er .\n",
      "--------------------------------------------------------------------------------\n",
      "PyTorch WARMUP SAMPLE 2\n",
      "SOURCE:    He asked himself what motive could have impelled Quasimodo to save her.\n",
      "TARGET:    Er fragte sich, welchen Beweggrund Quasimodo hätte haben können, sie zu retten.\n",
      "PRED:      Er fragte sich , was Quasimodo hätte haben können , um sie zu retten .\n",
      "\n",
      "PyTorch latency over 50 batches:\n",
      "  mean: 931.61 ms\n",
      "  p50 : 821.56 ms\n",
      "  p90 : 1729.10 ms\n",
      "  p99 : 2847.09 ms\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 1\n",
      "SOURCE:    All the long afternoon the village seemed empty and dead.\n",
      "TARGET:    Den ganzen langen Nachmittag schien das Dorf tot und verlassen.\n",
      "PRED:      Den ganzen Nachmittag schien das Dorf und der Nachmittag tot .\n",
      "--------------------------------------------------------------------------------\n",
      "TensorRT WARMUP SAMPLE 2\n",
      "SOURCE:    But Mary said, persuasively: \"Please, Tom--that's a good boy.\"\n",
      "TARGET:    Aber Mary sagte überredend: ,,Na, komm, Tom, sei ein braver Bursche!\"\n",
      "PRED:      Aber Mary sagte : ,, Bitte , Tom , ' s ist ein guter Junge !\"\n",
      "\n",
      "TensorRT latency over 50 batches:\n",
      "  mean: 709.41 ms\n",
      "  p50 : 452.08 ms\n",
      "  p90 : 830.60 ms\n",
      "  p99 : 6942.95 ms\n",
      "\n",
      "Speedup (PyTorch / TRT): 1.31x\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_len = config[\"seq_len\"]\n",
    "\n",
    "pt_times = benchmark_decode(\n",
    "    greedy_decode_pt, model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"PyTorch\",\n",
    "    print_warmup_samples=2\n",
    ")\n",
    "\n",
    "trt_times = benchmark_decode(\n",
    "    greedy_decode_trt, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    max_len=max_len,\n",
    "    device=device,\n",
    "    n_batches=50,\n",
    "    warmup_batches=5,\n",
    "    label=\"TensorRT\",\n",
    "    print_warmup_samples=2\n",
    ")\n",
    "\n",
    "speedup = np.mean(pt_times) / np.mean(trt_times)\n",
    "print(f\"\\nSpeedup (PyTorch / TRT): {speedup:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "941f0bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE:     The passage was tolerably long.\n",
      "TARGET:     Der Weg nach der Höhe war ziemlich lang.\n",
      "PT  PRED:   Der große Korridor war ziemlich lange .\n",
      "TRT PRED:   Der große Korridor war ziemlich lange .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def verbose_compare_same_sample(\n",
    "    pt_model,\n",
    "    trt_model,\n",
    "    dataloader,\n",
    "    tokenizer_src,\n",
    "    tokenizer_tgt,\n",
    "    max_len,\n",
    "    device,\n",
    "    batch_index=0,   # pick which batch to compare\n",
    "):\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # ---- get ONE specific batch ----\n",
    "    it = iter(dataloader)\n",
    "    batch = None\n",
    "    for i in range(batch_index + 1):\n",
    "        batch = next(it)\n",
    "\n",
    "    encoder_input = batch[\"encoder_input\"].to(device)\n",
    "    encoder_mask  = batch[\"encoder_mask\"].to(device)\n",
    "    source_text   = batch[\"src_text\"][0]\n",
    "    target_text   = batch[\"tgt_text\"][0]\n",
    "\n",
    "    # ---- PyTorch decode on SAME tensors ----\n",
    "    out_pt_ids = greedy_decode_pt(\n",
    "        pt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_pt_text = tokenizer_tgt.decode(out_pt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- TensorRT decode on SAME tensors ----\n",
    "    out_trt_ids = greedy_decode_trt(\n",
    "        trt_model, encoder_input, encoder_mask,\n",
    "        tokenizer_src, tokenizer_tgt,\n",
    "        max_len, device\n",
    "    )\n",
    "    out_trt_text = tokenizer_tgt.decode(out_trt_ids.detach().cpu().numpy())\n",
    "\n",
    "    # ---- print SAME sample ----\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"SOURCE:     {source_text}\")\n",
    "    print(f\"TARGET:     {target_text}\")\n",
    "    print(f\"PT  PRED:   {out_pt_text}\")\n",
    "    print(f\"TRT PRED:   {out_trt_text}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    return out_pt_ids, out_trt_ids\n",
    "\n",
    "\n",
    "# run on batch 0 (same sample)\n",
    "_ = verbose_compare_same_sample(\n",
    "    model, trt_model, val_dataloader,\n",
    "    tokenizer_src, tokenizer_tgt,\n",
    "    config[\"seq_len\"], device,\n",
    "    batch_index=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd97e13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First-step logits diff:\n",
      "  max abs: 0.005407452583312988\n",
      "  mean abs: 0.0012930561788380146\n"
     ]
    }
   ],
   "source": [
    "from dataset import causal_mask\n",
    "def compare_first_step_logits(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    # PT: encoder + one decoder step (SOS)\n",
    "    sos_idx = tokenizer_tgt.token_to_id(\"[SOS]\")\n",
    "    dec_in = torch.tensor([[sos_idx]], device=device, dtype=src.dtype)\n",
    "\n",
    "    enc_pt = pt_model.encode(src, src_mask)\n",
    "    dec_mask = causal_mask(1).type_as(src_mask).to(device)  # (1,1,1)\n",
    "    out_pt = pt_model.decode(enc_pt, src_mask, dec_in, dec_mask)\n",
    "    logits_pt = pt_model.project(out_pt[:, -1])  # (B, vocab)\n",
    "\n",
    "    # TRT: same\n",
    "    enc_trt = trt_model.encode(src, src_mask)\n",
    "    dec_mask_trt = causal_mask(1).type_as(src_mask).to(device).unsqueeze(1)  # (1,1,1,1)\n",
    "    out_trt = trt_model.decode(enc_trt, src_mask, dec_in, dec_mask_trt)\n",
    "    logits_trt = trt_model.project(out_trt[:, -1])\n",
    "\n",
    "    lp = logits_pt.detach().cpu().float()\n",
    "    lt = logits_trt.detach().cpu().float()\n",
    "\n",
    "    max_abs = (lp - lt).abs().max().item()\n",
    "    mean_abs = (lp - lt).abs().mean().item()\n",
    "    print(\"First-step logits diff:\")\n",
    "    print(\"  max abs:\", max_abs)\n",
    "    print(\"  mean abs:\", mean_abs)\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_first_step_logits(model, trt_model, batch0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc0b996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODER diff: max = 0.0012094676494598389 mean = 7.819623715477064e-05\n"
     ]
    }
   ],
   "source": [
    "def pt_encode_trt_style(pt_model, src, src_mask):\n",
    "    # Make PT use the same square mask shape TRT encoder expects\n",
    "    S = src.shape[1]\n",
    "    if src_mask.dim() == 3:\n",
    "        src_mask = src_mask.unsqueeze(1)   # (B,1,S)->(B,1,1,S)\n",
    "    if src_mask.dim() == 4 and src_mask.shape[2] == 1:\n",
    "        src_mask = src_mask.repeat(1,1,S,1)  # -> (B,1,S,S)\n",
    "    src_mask = src_mask.float()  # binary float\n",
    "    return pt_model.encode(src, src_mask)\n",
    "\n",
    "def compare_encoder_outputs(pt_model, trt_model, batch):\n",
    "    device = torch.device(\"cuda\")\n",
    "    pt_model.eval()\n",
    "    torch.set_grad_enabled(False)\n",
    "\n",
    "    src = batch[\"encoder_input\"].to(device)\n",
    "    src_mask = batch[\"encoder_mask\"].to(device)\n",
    "\n",
    "    enc_pt  = pt_encode_trt_style(pt_model, src, src_mask).detach().cpu().float()\n",
    "    enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "    diff = (enc_pt - enc_trt).abs()\n",
    "    print(\"ENCODER diff: max =\", diff.max().item(), \"mean =\", diff.mean().item())\n",
    "\n",
    "batch0 = next(iter(val_dataloader))\n",
    "compare_encoder_outputs(model, trt_model, batch0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e8351e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean diff padded   : 9.05511187738739e-05\n",
      "mean diff unpadded : 5.1022652769461274e-05\n",
      "max diff overall   : 0.00023682956816628575\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "batch0 = next(iter(val_dataloader))\n",
    "src = batch0[\"encoder_input\"].to(device)\n",
    "src_mask = batch0[\"encoder_mask\"].to(device)\n",
    "\n",
    "enc_pt  = pt_encode_trt_style(model, src, src_mask).detach().cpu().float()\n",
    "enc_trt = trt_model.encode(src, src_mask).detach().cpu().float()\n",
    "\n",
    "# per-position mean diff across hidden dim\n",
    "pos_diff = (enc_pt - enc_trt).abs().mean(-1).squeeze(0)   # (S,)\n",
    "\n",
    "# padded vs unpadded positions (mask is (1,1,1,S))\n",
    "key_mask = src_mask.squeeze().cpu()  # (S,) with 0/1\n",
    "pad_pos = key_mask == 0\n",
    "unpad_pos = key_mask == 1\n",
    "\n",
    "print(\"mean diff padded   :\", pos_diff[pad_pos].mean().item())\n",
    "print(\"mean diff unpadded :\", pos_diff[unpad_pos].mean().item())\n",
    "print(\"max diff overall   :\", pos_diff.max().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a98e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = min(len(pt_times), len(trt_times))\n",
    "df = pd.DataFrame({\n",
    "    \"pytorch_ms\": pt_times[:m],\n",
    "    \"tensorrt_ms\": trt_times[:m],\n",
    "})\n",
    "df[\"speedup_x\"] = df[\"pytorch_ms\"] / df[\"tensorrt_ms\"]\n",
    "\n",
    "df.to_csv(\"benchmark_times.csv\", index=False)\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debfaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36673b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "stty: 'standard input': Inappropriate ioctl for device\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: \"Messire,\" said Liénarde.\n",
      "    TARGET: »Herr,« sagte Liénarde.\n",
      " PREDICTED: » Herr ,« sagte Liénarde .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He picked up a clean pine shingle that lay in the moon-light, took a little fragment of \"red keel\" out of his pocket, got the moon on his work, and painfully scrawled these lines, emphasizing each slow down-stroke by clamping his tongue between his teeth, and letting up the pressure on the up-strokes. [See next page.]\n",
      "    TARGET: Er nahm eine glänzend geschliffene Schindel auf, die im Mondlicht lag, zog ein Stückchen Rotstift aus der Tasche, ließ das Mondlicht sein Werk bescheinen, und kritzelte mühsam, jeden schwerfälligen Grundstrich hervorhebend, indem er die Zunge zwischen die Zähne klemmte und sie bei den Haarstrichen wieder freiließ, folgende Zeilen: ,,Huck Finn und Tom Sawyer schwöhren, Sie wolen über dies den Mund Halten und sie wünschen, dahs Sie Tot niederfallen auff ihren Wech, wenn sie jemalls plautern oter schreiben.\"\n",
      " PREDICTED: Er nahm ein , , , , , ein wenig , das sich in der Tasche setzte . Dann nahm er seine Arbeit auf , legte die , die Zähne in die Finger des , die sich zwischen den Zähnen , die sich , die sich , die , die , , , , , , , .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Divide equally and give half the produce to labour, and the share left for you will be larger, and the labour force will receive more.\n",
      "    TARGET: Teilen Sie diesen Ertrag in zwei gleiche Teile und geben Sie die eine Hälfte an die Arbeiter; dann wird der Ihnen verbleibende Teil immer noch größer sein als bisher, und auch die Arbeiter werden mehr erhalten.\n",
      " PREDICTED: So und halb die Arbeit des , und Sie werden nicht mehr , und Sie werden mehr suchen als Arbeiter .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: You have been firing a good deal!' he said with a merry smile.\n",
      "    TARGET: Ihr habt ja so oft geschossen!« sagte er fröhlich lächelnd.\n",
      " PREDICTED: Du hast ja auch so viel Zeit !« sagte er mit einem hübschen Lächeln .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: I can't tell you just now what the moral of that is, but I shall remember it in a bit.'\n",
      "    TARGET: Ich kann dir diesen Augenblick nicht sagen, was die Moral davon ist, aber es wird mir gleich einfallen.«\n",
      " PREDICTED: Ich kann dir jetzt nicht sagen , was die Hauptsache ist , daß ich es in einem kleinen , kleinen Ding werde .«\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: 'Will you come and see me?' said the Countess after a pause. 'We must talk over something painful to you.\n",
      "    TARGET: »Kommen Sie heute zu mir«, sagte die Gräfin Lydia Iwanowna nach einem kurzen Stillschweigen, »wir müssen über eine für Sie sehr traurige Angelegenheit sprechen.\n",
      " PREDICTED: » Willst du mich denn einmal sehen ?« fragte die Gräfin Lydia Iwanowna nach einer Pause . » Wir müssen über etwas Neues sprechen .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: K. expressed himself in this curt way without any thought, so he was glad when the painter did not take this amiss and picked up a second painting from the floor.\n",
      "    TARGET: K. hatte unbedacht sich so kurz geäußert, er war daher froh, als der Maler, statt dies übelzunehmen, ein zweites Bild vom Boden aufhob.\n",
      " PREDICTED: K . stellte sich in diesem Augenblicke so kurz vor , ohne daß er sich selbst zu freuen glaubte , den Maler und den Eintritt in einen Teppich aus dem Boden .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: Diana intimated that this would be a different parting from any they had ever yet known.\n",
      "    TARGET: Diana deutete an, daß dies eine Trennung sein würde, sehr verschieden von jeder bisherigen.\n",
      " PREDICTED: Diana erklärte , daß dies eine andere für eine andere Person wäre , die sie noch niemals gekannt hätte .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: The conversation passed on to the abuses in the government of the United States, but Anna quickly turned it to another theme so as to draw the steward out of his silence.\n",
      "    TARGET: Das Gespräch ging nun zum Mißbrauch der Amtsgewalt in den Vereinigten Staaten über; aber Anna brachte es schleunigst auf ein anderes Gebiet, um auch den Verwalter zum Mitreden zu veranlassen.\n",
      " PREDICTED: Die Unterhaltung ging nach den in den des auf , die aller Art der , und Anna wandte sich schnell nach dem Verwalter , um den Verwalter zu nehmen .\n",
      "--------------------------------------------------------------------------------\n",
      "    SOURCE: He gathered quite a following of lads interested in the exhibition; and one that had cut his finger and had been a centre of fascination and homage up to this time, now found himself suddenly without an adherent, and shorn of his glory.\n",
      "    TARGET: Bald hatte er ein ganzes Gefolge, das seinen Vorführungen mit höchstem Interesse beiwohnte. Und einer mit einem geschnittenen Finger, der bisher der Mittelpunkt der Verehrung und Bewunderung gewesen war, sah sich auf einmal ohne Anhänger und seines Glanzes beraubt.\n",
      " PREDICTED: Er begann eine Menge von den Bauern zu trennen , um seinen Kameraden zu machen , und dieser , der ein Finger auf dem Rücken lag , hatte sich jetzt in diesem Augenblicke , ohne daß er sich selbst zu diesem Ruhm und seinem Wesen gefunden hätte .\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: print(msg), 0, None, num_examples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31231d48",
   "metadata": {},
   "source": [
    "# Checking the tensorboard files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b66b1699",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorboard as tb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd3d6f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.20.0 at http://localhost:6005/ (Press CTRL+C to quit)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If you are on ssh then make sure you are doing port forwarding\n",
    "#ssh -L 6005:localhost:6005 user@jetson_ip\n",
    "#Finally on your host on a browser open http://localhost:6005\n",
    "\n",
    "logdir = \"./runs/tmodel/\"\n",
    "os.system(f\"tensorboard --logdir {logdir} --port 6005\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv_jetson",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
